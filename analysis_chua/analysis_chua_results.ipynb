{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chua Faithfulness Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows: 1620\n",
            "Columns: ['question_with_cue', 'answer_due_to_cue', 'original_answer', 'ground_truth', 'cue_type', 'judge_extracted_evidence', 'cued_raw_response', 'model', 'cond']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('/Users/yuliav/PycharmProjects/thought-anchors-faithfulness/data/Chua_faithfulness_results.csv')\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unique Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Unique Models ===\n",
            "  qwen/qwq-32b-preview: 361 rows\n",
            "  gemini-2.0-flash-thinking-exp-01-21: 353 rows\n",
            "  deepseek-reasoner: 286 rows\n",
            "  qwen/qwen-2.5-72b-instruct: 89 rows\n",
            "  meta-llama/llama-3.3-70b-instruct: 108 rows\n",
            "  gpt-4o: 76 rows\n",
            "  claude-3-5-sonnet-20241022: 128 rows\n",
            "  gemini-2.0-flash-exp: 92 rows\n",
            "  x-ai/grok-2-1212: 127 rows\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Unique Models ===\")\n",
        "models = df['model'].unique()\n",
        "for m in models:\n",
        "    count = len(df[df['model'] == m])\n",
        "    print(f\"  {m}: {count} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models with Accessible Attention\n",
        "\n",
        "| Model | Attention Accessible | Notes |\n",
        "|-------|---------------------|-------|\n",
        "| `qwen/qwq-32b-preview` | ✅ Yes | 32B reasoning model |\n",
        "| `qwen/qwen-2.5-72b-instruct` | ✅ Yes | 72B |\n",
        "| `meta-llama/llama-3.3-70b-instruct` | ✅ Yes | 70B |\n",
        "| `gpt-4o` | ❌ No | API only |\n",
        "| `claude-3-5-sonnet-20241022` | ❌ No | API only |\n",
        "| `gemini-*` | ❌ No | API only |\n",
        "| `deepseek-reasoner` | ❌ No | API only |\n",
        "| `x-ai/grok-2-1212` | ❌ No | API only |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Response Length Analysis (All Models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== cued_raw_response stats (all models) ===\n",
            "Max char length: 32767\n",
            "Mean char length: 6216\n",
            "Median char length: 4325\n",
            "95th percentile: 16845\n",
            "99th percentile: 23156\n",
            "\n",
            "Model with max length response: deepseek-reasoner\n"
          ]
        }
      ],
      "source": [
        "# Add character length column\n",
        "df['char_len'] = df['cued_raw_response'].astype(str).str.len()\n",
        "\n",
        "print(\"=== cued_raw_response stats (all models) ===\")\n",
        "print(f\"Max char length: {df['char_len'].max()}\")\n",
        "print(f\"Mean char length: {df['char_len'].mean():.0f}\")\n",
        "print(f\"Median char length: {df['char_len'].median():.0f}\")\n",
        "print(f\"95th percentile: {df['char_len'].quantile(0.95):.0f}\")\n",
        "print(f\"99th percentile: {df['char_len'].quantile(0.99):.0f}\")\n",
        "\n",
        "# Find model with max length\n",
        "max_idx = df['char_len'].idxmax()\n",
        "print(f\"\\nModel with max length response: {df.loc[max_idx, 'model']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Response Length by Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>median</th>\n",
              "      <th>max</th>\n",
              "      <th>p95</th>\n",
              "      <th>p99</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claude-3-5-sonnet-20241022</th>\n",
              "      <td>128</td>\n",
              "      <td>1067</td>\n",
              "      <td>1129</td>\n",
              "      <td>1472</td>\n",
              "      <td>1390</td>\n",
              "      <td>1437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>deepseek-reasoner</th>\n",
              "      <td>286</td>\n",
              "      <td>10264</td>\n",
              "      <td>8832</td>\n",
              "      <td>32767</td>\n",
              "      <td>23514</td>\n",
              "      <td>32767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gemini-2.0-flash-exp</th>\n",
              "      <td>92</td>\n",
              "      <td>1777</td>\n",
              "      <td>1870</td>\n",
              "      <td>5030</td>\n",
              "      <td>2998</td>\n",
              "      <td>3476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gemini-2.0-flash-thinking-exp-01-21</th>\n",
              "      <td>353</td>\n",
              "      <td>10268</td>\n",
              "      <td>10122</td>\n",
              "      <td>19973</td>\n",
              "      <td>17015</td>\n",
              "      <td>19503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt-4o</th>\n",
              "      <td>76</td>\n",
              "      <td>1558</td>\n",
              "      <td>1578</td>\n",
              "      <td>3146</td>\n",
              "      <td>2236</td>\n",
              "      <td>3002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>meta-llama/llama-3.3-70b-instruct</th>\n",
              "      <td>108</td>\n",
              "      <td>2210</td>\n",
              "      <td>2214</td>\n",
              "      <td>4191</td>\n",
              "      <td>3502</td>\n",
              "      <td>3896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>qwen/qwen-2.5-72b-instruct</th>\n",
              "      <td>89</td>\n",
              "      <td>1944</td>\n",
              "      <td>1976</td>\n",
              "      <td>3398</td>\n",
              "      <td>2868</td>\n",
              "      <td>3275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>qwen/qwq-32b-preview</th>\n",
              "      <td>361</td>\n",
              "      <td>6904</td>\n",
              "      <td>6206</td>\n",
              "      <td>19517</td>\n",
              "      <td>14688</td>\n",
              "      <td>17289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x-ai/grok-2-1212</th>\n",
              "      <td>127</td>\n",
              "      <td>1481</td>\n",
              "      <td>1384</td>\n",
              "      <td>10644</td>\n",
              "      <td>2526</td>\n",
              "      <td>3301</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     count   mean  median    max    p95    p99\n",
              "model                                                                         \n",
              "claude-3-5-sonnet-20241022             128   1067    1129   1472   1390   1437\n",
              "deepseek-reasoner                      286  10264    8832  32767  23514  32767\n",
              "gemini-2.0-flash-exp                    92   1777    1870   5030   2998   3476\n",
              "gemini-2.0-flash-thinking-exp-01-21    353  10268   10122  19973  17015  19503\n",
              "gpt-4o                                  76   1558    1578   3146   2236   3002\n",
              "meta-llama/llama-3.3-70b-instruct      108   2210    2214   4191   3502   3896\n",
              "qwen/qwen-2.5-72b-instruct              89   1944    1976   3398   2868   3275\n",
              "qwen/qwq-32b-preview                   361   6904    6206  19517  14688  17289\n",
              "x-ai/grok-2-1212                       127   1481    1384  10644   2526   3301"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Stats per model\n",
        "model_stats = df.groupby('model')['char_len'].agg(['count', 'mean', 'median', 'max', \n",
        "                                                    lambda x: x.quantile(0.95),\n",
        "                                                    lambda x: x.quantile(0.99)])\n",
        "model_stats.columns = ['count', 'mean', 'median', 'max', 'p95', 'p99']\n",
        "model_stats = model_stats.round(0).astype(int)\n",
        "model_stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt Length Analysis: Stanford Professor Questions\n",
        "\n",
        "Analyzing the `question_with_cue` column length for Stanford Professor questions to understand context window requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Prompt Length Stats (Stanford Professor Questions) ===\n",
            "Number of Professor questions: 199\n",
            "Max prompt length: 2454 characters\n",
            "Mean prompt length: 839 characters\n",
            "95th percentile: 2027 characters\n",
            "99th percentile: 2375 characters\n"
          ]
        }
      ],
      "source": [
        "# Filter for Stanford Professor questions\n",
        "professor_df = df[\n",
        "    (df['cue_type'] == 'Professor') & \n",
        "    (df['cond'].isin(['itc_failure', 'itc_success']))\n",
        "].copy()\n",
        "\n",
        "# Calculate prompt length (question_with_cue)\n",
        "professor_df['prompt_char_len'] = professor_df['question_with_cue'].astype(str).str.len()\n",
        "\n",
        "print(\"=== Prompt Length Stats (Stanford Professor Questions) ===\")\n",
        "print(f\"Number of Professor questions: {len(professor_df)}\")\n",
        "print(f\"Max prompt length: {professor_df['prompt_char_len'].max()} characters\")\n",
        "print(f\"Mean prompt length: {professor_df['prompt_char_len'].mean():.0f} characters\")\n",
        "print(f\"95th percentile: {professor_df['prompt_char_len'].quantile(0.95):.0f} characters\")\n",
        "print(f\"99th percentile: {professor_df['prompt_char_len'].quantile(0.99):.0f} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Token Estimates for Prompts ===\n",
            "Max prompt @ 3 chars/token: 818 tokens\n",
            "Max prompt @ 4 chars/token: 613 tokens\n",
            "\n",
            "Mean prompt @ 3 chars/token: 279.0 tokens\n",
            "Mean prompt @ 4 chars/token: 209.0 tokens\n",
            "\n",
            "95th percentile @ 3 chars/token: 675.0 tokens\n",
            "95th percentile @ 4 chars/token: 506.0 tokens\n"
          ]
        }
      ],
      "source": [
        "# Token estimates for prompts\n",
        "max_prompt_chars = professor_df['prompt_char_len'].max()\n",
        "mean_prompt_chars = professor_df['prompt_char_len'].mean()\n",
        "p95_prompt_chars = professor_df['prompt_char_len'].quantile(0.95)\n",
        "p99_prompt_chars = professor_df['prompt_char_len'].quantile(0.99)\n",
        "\n",
        "print(\"=== Token Estimates for Prompts ===\")\n",
        "print(f\"Max prompt @ 3 chars/token: {max_prompt_chars // 3} tokens\")\n",
        "print(f\"Max prompt @ 4 chars/token: {max_prompt_chars // 4} tokens\")\n",
        "print()\n",
        "print(f\"Mean prompt @ 3 chars/token: {mean_prompt_chars // 3} tokens\")\n",
        "print(f\"Mean prompt @ 4 chars/token: {mean_prompt_chars // 4} tokens\")\n",
        "print()\n",
        "print(f\"95th percentile @ 3 chars/token: {p95_prompt_chars // 3} tokens\")\n",
        "print(f\"95th percentile @ 4 chars/token: {p95_prompt_chars // 4} tokens\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Context Window Requirements\n",
        "\n",
        "For `generate_rollouts.py` with `max_model_len` setting:\n",
        "- Need to accommodate: **prompt length + max_tokens (generated response)**\n",
        "- Current setting: `max_model_len=2048` tokens\n",
        "- If using `max_tokens=2048` for generation, total needed = prompt_tokens + 2048\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ITC Success (Faithful Switch)\n",
        "\n",
        "- The model **switches its answer toward the cue**\n",
        "- The model **explicitly acknowledges** that the cue influenced its decision\n",
        "\n",
        "#### ITC Failure (Unfaithful Switch)\n",
        "\n",
        "- The model **switches its answer toward the cue**\n",
        "- BUT **does NOT acknowledge** the cue in its reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== cond Values for Professor Questions ===\n",
            "cond\n",
            "itc_success         162\n",
            "non_itc_success      49\n",
            "non_itc_failures     38\n",
            "itc_failure          37\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Total Professor questions (all cond): 286\n"
          ]
        }
      ],
      "source": [
        "    \n",
        "\n",
        "# Check cond values for Professor questions specifically\n",
        "print(\"=== cond Values for Professor Questions ===\")\n",
        "prof_all = df[df['cue_type'] == 'Professor']\n",
        "print(prof_all['cond'].value_counts())\n",
        "print()\n",
        "print(f\"Total Professor questions (all cond): {len(prof_all)}\")\n",
        "\n",
        "# Check what we get with itc_failure + itc_success filter\n",
        "prof_filtered = df[\n",
        "    (df['cue_type'] == 'Professor') & \n",
        "    (df['cond'].isin(['itc_failure', 'itc_success']))\n",
        "]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
