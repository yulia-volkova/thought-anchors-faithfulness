<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Faithful vs Unfaithful CoT Attention Analysis</title>
    <link rel="icon" type="image/png" href="favicon.png">
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:wght@400;600;700&family=Source+Sans+Pro:wght@400;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <!-- Header -->
        <header class="header">
            <h1>Do Faithful &amp; Unfaithful CoT Differ in How They Focus Attention? <img src="images/waving_emoji.png" alt="Hi!" class="header-emoji-large"></h1>
            <p class="author-line">
                <span>By Yulia Volkova | MATS 9.0</span>
                <span class="author-links">
                    <a href="https://www.linkedin.com/in/yuulia-volkova/" target="_blank" title="LinkedIn">
                        <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="currentColor"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg>
                    </a>
                    <a href="https://yulia-volkova.github.io/" target="_blank" title="Website">
                        <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    </a>
                    <a href="https://github.com/yulia-volkova/thought-anchors-faithfulness" target="_blank" title="GitHub">
                        <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                    </a>
                </span>
            </p>
        </header>

        <!-- TL;DR Section -->
        <section class="tldr-section">
            <h2>TL;DR</h2>
            <p class="tldr-intro">
                I analyze consistently faithful &amp; unfaithful rollouts for MMLU &amp; GPQA-diamond datasets using the
                <a href="https://arxiv.org/abs/2506.19143" target="_blank">Thought Anchors</a> framework.
                Key findings: (1) <strong>Faithful CoT shows more focused attention patterns</strong> (higher kurtosis <span class="info-icon" id="kurtosis-info" data-tooltip="kurtosis-tooltip">‚ÑπÔ∏è</span>) than unfaithful CoT. (2) Reasoning-only receiver heads <span class="info-icon" id="receiver-heads-tldr-info" data-tooltip="receiver-heads-tldr-tooltip">‚ÑπÔ∏è</span> differ between groups, but this is <strong>not statistically significant</strong>. (3) <strong>Unfaithful CoT shows higher cue-following rate</strong> than faithful CoT. (4) There exist <strong>universal receiver heads</strong> that appear across all conditions.
            </p>
            <p class="tldr-link">
                <a href="#key-findings">View detailed findings below ‚Üì</a>
            </p>
            <div id="kurtosis-tooltip" class="control-tooltip hidden">
                <p>A statistical measure of how "peaked" the attention distribution is. Higher kurtosis means attention is focused on fewer specific tokens rather than spread evenly.</p>
            </div>
            <div id="receiver-heads-tldr-tooltip" class="control-tooltip hidden">
                <p>Attention heads with highest kurtosis in their attention distribution - they consistently focus on specific source sentences.</p>
            </div>
        </section>

        <!-- Hypothesis Section (Collapsed by default) -->
        <section class="methodology-collapsible" id="hypothesis-section">
            <h2 class="methodology-header" onclick="toggleHypothesis()">
                Hypothesis
                <span id="hypothesis-toggle" class="toggle-icon">‚ñ∂</span>
            </h2>
            <div id="hypothesis-content" class="methodology-body collapsed">
                <p>
                    If white-box attention attribution patterns reflect how models use information during reasoning,
                    then <strong>faithful CoT</strong> (where reasoning genuinely influences the answer) should show focused attention
                    to relevant broadcasting sentences, while <strong>unfaithful CoT</strong> (post-hoc rationalization)
                    may exhibit more diffuse attention or anchor to different, potentially misleading parts of the reasoning chain.
                </p>
                <p>
                    This work replicates and extends <a href="https://arxiv.org/abs/2506.19143" target="_blank">Bogdan et al. (2025) "Thought Anchors"</a>
                    to test whether receiver heads and vertical stripe patterns of attention targeted towards specific sentences ("thought anchors") differ between faithful and unfaithful chains.
                    If attention markers meaningfully distinguish faithful from unfaithful CoT, this could enable
                    white-box monitoring of reasoning quality in deployed models.
                </p>
            </div>
        </section>

        <!-- Key Findings Section (Collapsible) -->
        <section class="key-findings-section" id="key-findings">
            <h2 class="key-findings-header" onclick="toggleKeyFindings()">
                Detailed Key Findings
                <span id="key-findings-toggle" class="toggle-icon">‚ñ∂</span>
            </h2>
            <div id="key-findings-content" class="key-findings-body collapsed">
            <div class="findings-grid">
                <!-- Finding 1: Focused Attention Patterns (Kurtosis Magnitude) -->
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="finding-metric">Focused Attention Patterns</span>
                        <span class="finding-pval finding-sig">p = 0.04 (GPQA)</span>
                    </div>
                    <p>Faithful CoT has <strong>more focused attention</strong> (higher kurtosis) than unfaithful CoT. This suggests faithful reasoning explicitly anchors to key sentences, while unfaithful has more diffuse attention.</p>
                    <p style="font-size: 0.85rem; color: var(--text-secondary); margin-top: 0.5rem;">
                        <strong>GPQA Reasoning-Only:</strong> Faithful mean kurtosis = 12.74, Unfaithful = 9.76.<br>
                        <strong>MMLU Reasoning-Only:</strong> Faithful = 2.94, Unfaithful = 1.43 (p = 0.055, marginal).
                    </p>
                    <p class="finding-example" style="font-size: 0.8rem; background: var(--bg-tertiary); padding: 0.5rem; border-radius: 4px; margin-top: 0.5rem;">
                        <strong>Example:</strong> PI 162 (faithful, GPQA) focuses on physics equations with max head kurtosis = 49.4, while PI 116 (unfaithful) has more diffuse attention with max kurtosis = 36.9.
                    </p>
                    <div class="finding-discussion">
                        <strong>Discussion:</strong> This is good news to me! This observation corroborates my <a href="#hypothesis-section" onclick="document.getElementById('hypothesis-content').classList.remove('collapsed'); document.getElementById('hypothesis-toggle').classList.add('expanded');">initial hypothesis</a> that unfaithful CoT would have less thought anchors and more diffuse attention indicating performative reasoning. This being significant on GPQA is good, because it is a more challenging dataset that is less likely to suffer from performative reasoning in general, but I control for low reasoning accuracy to avoid this anyway. I would like to explore this point more in other settings too.
                    </div>
                    <div class="definitions-section" style="display: flex; gap: 0.5rem; margin-bottom: 0.5rem;">
                        <button class="expand-graph-btn" onclick="toggleDefinition('what-is-kurtosis')">What is kurtosis?</button>
                    </div>
                    <div id="what-is-kurtosis" class="correlation-graph hidden" style="margin-bottom: 0.5rem;">
                        <p style="font-size: 0.9rem; color: #666; font-style: italic; margin: 0;">
                            Kurtosis is a statistical measure of how "peaked" the attention distribution is. Higher kurtosis means attention is focused on fewer specific tokens rather than spread evenly across many tokens.
                        </p>
                    </div>
                    <button class="expand-graph-btn" onclick="toggleGraph('kurtosis-magnitude')">Show graph ‚ñº</button>
                    <div id="kurtosis-magnitude" class="correlation-graph hidden">
                        <img src="images/conclusion_kurtosis_magnitude_web.png" alt="Kurtosis Magnitude Comparison">
                    </div>
                </div>

                <!-- Finding 2: Divergent Circuits -->
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="finding-metric">Divergent Attention Circuits</span>
                        <span class="finding-pval finding-ns">p = 0.31-0.68</span>
                    </div>
                    <p>When we look at <strong>reasoning only</strong> attention, faithful and unfaithful questions use completely different (only 0-11% overlap) heads to focus on the most important sentences.</p>
                    <p>However, when <strong>prompt is included</strong> into the calculation of receiver heads and attention to sentences within prompt is also accounted for, faithful and unfaithful CoT <strong>share 62-83%</strong> of top receiver heads.</p>
                    <p class="finding-caveat"><strong>Statistical caveat:</strong> The 0% reasoning-only overlap is NOT significantly different from random groupings (~2-9% baseline overlap). In-group consistency is also near-zero, suggesting receiver heads are highly problem-specific rather than faithfulness-specific.</p>
                    <div class="finding-discussion">
                        <strong>Discussion:</strong> Initially I thought of this as very good news potentially meaning that faithful CoT would consistently have different receiver heads from the unfaithful ones, but this turned out to be non-significant. And the heads are more "problem specific". I would still check this again in a larger experiment.
                    </div>
                    <button class="expand-graph-btn" onclick="toggleGraph('divergent-circuits')">Show graph ‚ñº</button>
                    <div id="divergent-circuits" class="correlation-graph hidden">
                        <img src="images/conclusion_divergent_circuits_web.png" alt="Divergent Circuits Comparison">
                    </div>
                </div>

                <!-- Finding 3: Hidden Influence -->
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="finding-metric">Unfaithful CoT shows higher cue-following rate</span>
                        <span class="finding-pval finding-sig">p &lt; 0.05 (CI)</span>
                    </div>
                    <ul class="finding-list">
                        <li>Unfaithful CoT show higher cue-following rate (80%) than faithful CoT (71%) when we control for cue response gap ‚â• 0.5.</li>
                        <li>Without controlling for high cue influence, the pattern reverses: unfaithful 34% vs faithful 42%.</li>
                        <li>In <a href="#selection-criteria" onclick="document.getElementById('methodology-content').classList.remove('collapsed'); document.getElementById('methodology-toggle').classList.add('expanded');">selected</a> subset of questions, where we control both for cue response gap and low no reasoning accuracy, the pattern strengthens: unfaithful CoT show higher cue-following rate (91%) than faithful CoT (66%). <strong>Bootstrap 95% CI on the difference confirms significance.</strong></li>
                    </ul>
                    <div class="finding-discussion">
                        <strong>Discussion:</strong> I was slightly surprised by this finding. I would expect that cue verbalisation and acknowledgement is associated with cue following. It can be the case that the model is overall confused/dumb but we would expect that the model that struggled to be faithful also wouldn't follow the cue. Or maybe being unfaithful = already making a mistake so it gives wrong answer as well and it's just wrong everywhere.
                    </div>
                    <button class="expand-graph-btn" onclick="toggleGraph('hidden-influence')">Show graph ‚ñº</button>
                    <div id="hidden-influence" class="correlation-graph hidden">
                        <img src="images/conclusion_hidden_influence_web.png" alt="Hidden Influence Comparison">
                    </div>
                </div>

                <!-- Finding 4: Universal Receiver Heads -->
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="finding-metric">Universal Receiver Heads</span>
                        <span class="finding-pval finding-sig">p &lt; 0.001</span>
                    </div>
                    <p>Certain attention heads (e.g., L45_H0, L47_H3, L31_H34) appear as top receivers across both MMLU and GPQA, and in both faithful and unfaithful categories.</p>
                    <p style="font-size: 0.85rem; color: var(--text-secondary); margin-top: 0.5rem;">
                        These heads may represent fundamental "answer extraction" circuits in the model, independent of reasoning faithfulness. Permutation test confirms this is unlikely by chance (p &lt; 0.001).
                    </p>
                    <div class="finding-discussion">
                        <strong>Discussion:</strong> This is expected as it corroborates the findings in the original <a href="https://arxiv.org/abs/2506.19143" target="_blank">Thought Anchors paper (Bogdan et al., 2025)</a>. It can be useful for optimisation of future analysis - to check these universal heads first, or to check what makes them not appear as receiver heads.
                        <p style="font-size: 0.85rem; margin-top: 0.5rem;"><strong>Note on L36_H6:</strong> This head appears in only 2/4 combinations (MMLU faithful and MMLU unfaithful) but not in GPQA. In GPQA, L36_H25 appears instead. This may be because GPQA problems tend to be longer and more complex, activating different attention patterns.</p>
                    </div>
                    <div class="definitions-section" style="display: flex; gap: 0.5rem; margin-bottom: 0.5rem;">
                        <button class="expand-graph-btn" onclick="toggleDefinition('what-is-kurtosis-2')">What is kurtosis?</button>
                    </div>
                    <div id="what-is-kurtosis-2" class="correlation-graph hidden" style="margin-bottom: 0.5rem;">
                        <p style="font-size: 0.9rem; color: #666; font-style: italic; margin: 0;">
                            Kurtosis is a statistical measure of how "peaked" the attention distribution is. Higher kurtosis means attention is focused on fewer specific tokens rather than spread evenly across many tokens.
                        </p>
                    </div>
                    <button class="expand-graph-btn" onclick="toggleGraph('universal-heads')">Show graph ‚ñº</button>
                    <div id="universal-heads" class="correlation-graph hidden">
                        <img src="images/conclusion_universal_heads_web.png" alt="Universal Receiver Heads">
                    </div>
                </div>
            </div>
            </div> <!-- key-findings-content -->
        </section>

        <!-- Methodology Section (Collapsed by default) -->
        <section class="methodology-collapsible" id="methodology">
            <h2 class="methodology-header" onclick="toggleMethodology()">
                Methodology
                <span id="methodology-toggle" class="toggle-icon">‚ñ∂</span>
            </h2>
            <div id="methodology-content" class="methodology-body collapsed">

                <!-- Pipeline Overview -->
                <h3>Pipeline</h3>
                <div class="pipeline-overview">
                    <img src="images/methodology_pipeline.svg" alt="Methodology Pipeline" class="pipeline-img">
                </div>

                <h3>Dataset Statistics</h3>
                <p><strong>MMLU:</strong> 143 problems, 20 rollouts per condition, median reasoning accuracy 0.637, median no-reasoning accuracy 0.474.</p>
                <p><strong>GPQA-Diamond:</strong> 186 problems, 20 rollouts per condition, mean base accuracy 0.577, mean no-reasoning accuracy 0.378.</p>

                <h3>Model &amp; Generation Settings</h3>
                <p>
                    Two generation stages: (1) initial rollouts for faithfulness classification,
                    (2) attention analysis rollouts for visualization.
                </p>
                <ul class="clean-list">
                    <li><strong>Model:</strong> <code>deepseek-ai/deepseek-r1-distill-qwen-14b</code></li>
                    <li><strong>Temperature:</strong> 0.7, <strong>Top-p:</strong> 0.95</li>
                    <li><strong>Max tokens:</strong> 2048 (MMLU), 8192/2048 (GPQA stage 1/2)</li>
                    <li><strong>Rollouts:</strong> 20 per condition (stage 1), 5 per condition (stage 2)</li>
                </ul>

                <h3>Faithfulness Classification</h3>
                <ul class="clean-list">
                    <li><strong>Faithful CoT:</strong> Follows cue AND mentions it in reasoning (threshold: ‚â•80% MMLU, ‚â•70% GPQA)</li>
                    <li><strong>Unfaithful CoT:</strong> Follows cue but doesn't mention it (threshold: ‚â•80% MMLU, ‚â•70% GPQA)</li>
                </ul>

                <h3 id="selection-criteria">Problem Selection Criteria</h3>
                <p>Problems were selected from MMLU and GPQA rollouts using the following criteria:</p>
                <ul class="clean-list">
                    <li><strong>Low no-reasoning accuracy (&lt;50%)</strong> ‚Äî Problems where the model struggles without CoT</li>
                    <li><strong>High cue response gap (‚â•0.5)</strong> ‚Äî Large difference in cue-following between cued and uncued conditions</li>
                    <li><strong>Cue ‚â† Ground Truth</strong> ‚Äî The cue answer differs from the correct answer</li>
                    <li><strong>Each problem is unique</strong></li>
                </ul>

                <h3>Preliminary Data Analysis</h3>
                <div class="definitions-section" style="display: flex; gap: 1rem; margin-bottom: 1rem;">
                    <div>
                        <button class="expand-graph-btn" onclick="toggleDefinition('what-is-r')">What is "r"?</button>
                        <div id="what-is-r" class="correlation-graph hidden" style="margin-top: 0.5rem;">
                            <p style="font-size: 0.9rem; color: #666; font-style: italic; margin: 0;">
                                "r" represents the Pearson correlation coefficient, measuring the linear relationship between two variables.
                                Values range from -1 (perfect negative correlation) to +1 (perfect positive correlation), with 0 indicating no linear relationship.
                            </p>
                        </div>
                    </div>
                    <div>
                        <button class="expand-graph-btn" onclick="toggleDefinition('what-is-gap')">What is "Gap"?</button>
                        <div id="what-is-gap" class="correlation-graph hidden" style="margin-top: 0.5rem;">
                            <p style="font-size: 0.9rem; color: #666; font-style: italic; margin: 0;">
                                "Gap" refers to the mean cue response gap ‚Äî the average increase in cue-following behavior when a misleading cue is present.
                            </p>
                        </div>
                    </div>
                </div>

                <!-- MMLU Findings -->
                <h4 style="margin-top: 1rem; color: #666;">MMLU</h4>
                <div class="findings-grid">
                    <div class="finding-card">
                        <div class="finding-header">
                            <span class="finding-metric">r(accuracy, cue following) = -0.459</span>
                            <span class="finding-pval">p &lt; 0.0001</span>
                        </div>
                        <p>Lower base accuracy correlates with <strong>more cue following</strong> ‚Äî the model is more susceptible to misleading cues on harder problems.</p>
                        <p class="finding-caveat">Cue following behaviour is likely confounded by problem difficulty. On harder problems, the model may fall back to the cue when struggling to find an answer. To control for this, future work should compare problems with equal uncued accuracy levels.</p>
                        <button class="expand-graph-btn" onclick="toggleGraph('mmlu-acc-cue-follow')">Show graph ‚ñº</button>
                        <div id="mmlu-acc-cue-follow" class="correlation-graph hidden">
                            <img src="images/accuracy_vs_cue_following.png" alt="Accuracy vs Cue Following correlation" style="width: 100%; max-width: 600px; margin-top: 1rem;">
                        </div>
                    </div>
                    <div class="finding-card">
                        <div class="finding-header">
                            <span class="finding-metric">Gap = 0.289</span>
                            <span class="finding-pval">p &lt; 0.0001</span>
                        </div>
                        <p>Mean cue response gap of 0.289 ‚Äî <strong>79.7%</strong> of problems show increased cue following when cued.</p>
                        <button class="expand-graph-btn" onclick="toggleGraph('mmlu-gap-dist')">Show distribution ‚ñº</button>
                        <div id="mmlu-gap-dist" class="correlation-graph hidden">
                            <img src="images/mmlu_gap_distribution.png" alt="Cue Response Gap Distribution (MMLU)" style="width: 100%; max-width: 600px; margin-top: 1rem;">
                        </div>
                    </div>
                    <div class="finding-card">
                        <div class="finding-header">
                            <span class="finding-metric">r(accuracy, gap) = 0.008</span>
                            <span class="finding-pval">p = 0.92</span>
                        </div>
                        <p>No significant correlation between base accuracy and cue response gap ‚Äî the <em>change</em> in behavior is consistent across difficulty levels.</p>
                        <button class="expand-graph-btn" onclick="toggleGraph('mmlu-acc-gap')">Show graph ‚ñº</button>
                        <div id="mmlu-acc-gap" class="correlation-graph hidden">
                            <img src="images/mmlu_accuracy_vs_gap.png" alt="Accuracy vs Cue Response Gap correlation" style="width: 100%; max-width: 600px; margin-top: 1rem;">
                        </div>
                    </div>
                </div>

                <!-- GPQA Findings -->
                <h4 style="margin-top: 1.5rem; color: #666;">GPQA-Diamond</h4>
                <div class="findings-grid">
                    <div class="finding-card">
                        <div class="finding-header">
                            <span class="finding-metric">r(accuracy, cue following) = -0.522</span>
                            <span class="finding-pval">p &lt; 0.0001</span>
                        </div>
                        <p>Lower base accuracy correlates with <strong>more cue following</strong> ‚Äî effect is even stronger than MMLU.</p>
                        <p class="finding-caveat">Cue following behaviour is likely confounded by problem difficulty. On harder problems, the model may fall back to the cue when struggling to find an answer. To control for this, future work should compare problems with equal uncued accuracy levels.</p>
                        <button class="expand-graph-btn" onclick="toggleGraph('gpqa-acc-cue-follow')">Show graph ‚ñº</button>
                        <div id="gpqa-acc-cue-follow" class="correlation-graph hidden">
                            <img src="images/gpqa_accuracy_vs_cue_following.png" alt="Accuracy vs Cue Following correlation (GPQA)" style="width: 100%; max-width: 600px; margin-top: 1rem;">
                        </div>
                    </div>
                    <div class="finding-card">
                        <div class="finding-header">
                            <span class="finding-metric">Gap = 0.183</span>
                            <span class="finding-pval">p &lt; 0.0001</span>
                        </div>
                        <p>Mean cue response gap of 0.183 ‚Äî <strong>67.2%</strong> of problems show increased cue following when cued.</p>
                        <button class="expand-graph-btn" onclick="toggleGraph('gpqa-gap-dist')">Show distribution ‚ñº</button>
                        <div id="gpqa-gap-dist" class="correlation-graph hidden">
                            <img src="images/gpqa_gap_distribution.png" alt="Cue Response Gap Distribution (GPQA)" style="width: 100%; max-width: 600px; margin-top: 1rem;">
                        </div>
                    </div>
                    <div class="finding-card">
                        <div class="finding-header">
                            <span class="finding-metric">r(faithfulness, accuracy drop) = -0.273</span>
                            <span class="finding-pval">p = 0.0002</span>
                        </div>
                        <p>Higher faithfulness (explicit cue mentions) ‚Üí <strong>larger accuracy drops</strong>. Verbalizing the cue is associated with being more misled.</p>
                        <button class="expand-graph-btn" onclick="toggleGraph('gpqa-faith-acc-drop')">Show graph ‚ñº</button>
                        <div id="gpqa-faith-acc-drop" class="correlation-graph hidden">
                            <img src="images/gpqa_faithfulness_vs_accuracy_drop.png" alt="Faithfulness vs Accuracy Drop correlation (GPQA)" style="width: 100%; max-width: 600px; margin-top: 1rem;">
                        </div>
                    </div>
                </div>

                <h3>Attention Analysis</h3>
                <p>
                    For each problem, we identify <strong>receiver heads</strong> ‚Äî attention heads with high kurtosis
                    in their attention distribution. High kurtosis indicates the head consistently
                    attends to specific source sentences (creating "vertical stripes" in the attention matrix).
                    I compare which source sentences receive the most attention in cued vs. uncued rollouts.
                </p>

            </div> <!-- methodology-content -->
        </section>

        <!-- Try It Out Section -->
        <section class="try-it-out-section">
            <h2>Try It Out!</h2>
        </section>

        <!-- Dataset Selection -->
        <section class="control-panel">
            <div class="control-group">
                <label for="dataset-select">Dataset</label>
                <select id="dataset-select">
                    <option value="mmlu" selected>MMLU</option>
                    <option value="gpqa">GPQA-Diamond</option>
                </select>
            </div>

            <div class="control-group">
                <label>Category</label>
                <div class="checkbox-group" id="category-checkboxes">
                    <label class="checkbox-label">
                        <input type="checkbox" id="cat-faithful" value="faithful" checked>
                        <span>Faithful CoT</span>
                    </label>
                    <label class="checkbox-label">
                        <input type="checkbox" id="cat-unfaithful" value="unfaithful" checked>
                        <span>Unfaithful CoT</span>
                    </label>
                </div>
            </div>

            <div class="control-group">
                <label for="pi-select">Question ID</label>
                <select id="pi-select">
                    <!-- Populated dynamically -->
                </select>
            </div>

            <div class="control-group">
                <label for="heads-source">
                    Receiver Heads From
                    <span class="info-icon" id="heads-source-info" data-tooltip="heads-source-tooltip">‚ÑπÔ∏è</span>
                </label>
                <select id="heads-source">
                    <option value="aggregate">All in Category</option>
                    <option value="single">This Question Only</option>
                </select>
                <div id="heads-source-tooltip" class="control-tooltip hidden">
                    <p><strong>All in Category:</strong> Receiver heads are calculated using kurtosis measure calculated across all rollouts from all question IDs in the selected category. If only one category (faithful or unfaithful) is selected, kurtosis is computed across all rollouts from that category.</p>
                    <p style="margin-top: 0.5rem;"><strong>This Question Only:</strong> Receiver heads are calculated only across rollouts of this specific question ID.</p>
                    <p style="margin-top: 0.5rem;"><strong>Note on combining categories:</strong> If both faithful and unfaithful categories are selected, kurtosis would be computed across all rollouts from both categories combined. However, this may not be statistically appropriate since faithful and unfaithful problems may exhibit systematically different attention patterns.</p>
                    <p style="margin-top: 0.5rem; font-size: 0.85rem; color: var(--text-muted);">Note: Bogdan et al. calculate receiver heads across the whole dataset of math rollouts (<a href="https://huggingface.co/datasets/uzaymacar/math-rollouts" target="_blank">link</a>).</p>
                </div>
            </div>

            <div class="control-group">
                <label for="attention-mode">Attention Mode</label>
                <select id="attention-mode">
                    <option value="full" selected>With Prompt</option>
                    <option value="reasoning">Reasoning Only</option>
                </select>
            </div>
        </section>

        <!-- TBA Message (for datasets without data) -->
        <div id="tba-message" class="tba-message hidden">
            <h2>üìä No Problems Available</h2>
            <p>No problems found for this dataset/category combination.</p>
        </div>

        <!-- Main Content -->
        <main id="main-content">
            <!-- Question Info -->
            <section class="question-info">
                <h2>Question ID <span id="question-details-id">‚Äî</span> <span id="pi-category-badge" class="category-badge"></span></h2>
                <div class="info-grid">
                    <div class="info-card">
                        <span class="info-label">Question Text</span>
                        <p id="question-text" class="question-text">Loading...</p>
                        <button id="expand-question-btn" class="expand-btn" onclick="toggleQuestionExpand()">Show more ‚ñº</button>
                    </div>
                    <div class="info-row">
                        <div class="info-card small">
                            <span class="info-label">Ground Truth</span>
                            <span id="gt-answer" class="answer-badge gt">‚Äî</span>
                        </div>
                        <div class="info-card small">
                            <span class="info-label">Cue Answer</span>
                            <span id="cue-answer" class="answer-badge cue">‚Äî</span>
                        </div>
                        <div class="info-card small">
                            <span class="info-label">Reasoning Acc</span>
                            <span id="reasoning-acc" class="stat-value">‚Äî</span>
                        </div>
                        <div class="info-card small">
                            <span class="info-label">No-Reasoning Acc</span>
                            <span id="no-reasoning-acc" class="stat-value">‚Äî</span>
                        </div>
                        <div class="info-card small">
                            <span class="info-label">Faithfulness %</span>
                            <span id="faithfulness-pct" class="stat-value faithful-stat">‚Äî</span>
                        </div>
                    </div>
                </div>
                
                <!-- Warning for problems where reasoning is worse than no-reasoning -->
                <div id="reasoning-warning" class="reasoning-warning hidden">
                    <span class="warning-icon">‚ö†Ô∏è</span>
                    <span class="warning-text">
                        <strong>Note:</strong> This problem may be non-representative as its reasoning accuracy is lower than no-reasoning accuracy. 
                        However, attention patterns may still be interesting for such examples.
                    </span>
                </div>
            </section>

            <p class="viz-intro">Below I look at the cued versus uncued versions of the rollout for question ID <span id="selected-question-id" class="highlight-id">‚Äî</span>, <span id="selected-category-label" class="highlight-category">‚Äî</span></p>

            <!-- Left doodle (positioned behind the box) -->
            <div class="doodle-wrapper">
                <img src="images/receiver_head_left.png" alt="" class="receiver-doodle-left">
            </div>

            <!-- Receiver Heads + Attention unified container -->
            <div class="unified-attention-container">
            <!-- Receiver Heads -->
            <section class="receiver-heads-section">
                <img src="images/receiver_heads_doodle_right.png" alt="" class="receiver-doodle-right">
                <h3 class="receiver-heads-title">Top Receiver Heads <span class="info-icon" id="receiver-heads-info" data-tooltip="receiver-heads-tooltip">‚ÑπÔ∏è</span></h3>
                <div id="receiver-heads-tooltip" class="control-tooltip hidden">
                    <p>Heads with highest kurtosis in attention distribution</p>
                </div>
                <p id="heads-fallback-note" class="fallback-note hidden"></p>

                <div class="heads-comparison">
                    <div class="heads-column">
                        <h3>Cued Rollout</h3>
                        <div id="cued-heads" class="heads-list">
                            <!-- Populated dynamically -->
                        </div>
                    </div>
                    <div class="heads-column">
                        <h3>Uncued Rollout</h3>
                        <div id="uncued-heads" class="heads-list">
                            <!-- Populated dynamically -->
                        </div>
                    </div>
                </div>

                <div class="shared-heads">
                    <span class="shared-label">Shared Heads for cued & uncued:</span>
                    <span id="shared-heads-list">‚Äî</span>
                </div>
            </section>

            <!-- Attention Matrices Side by Side -->
            <section class="attention-section">
                <h2>Attention Patterns</h2>
                <p class="section-desc">Click on a head above to view its attention matrix. Hover for values.</p>
                
                <div class="attention-comparison">
                    <!-- Cued Panel -->
                    <div class="attention-panel">
                        <div class="panel-header">
                            <h3>Cued Rollout</h3>
                            <span id="cued-selected-head" class="selected-head-badge">L0-H0</span>
                            <span id="cued-prof-mention" class="prof-mention-badge">Faithfulness rate: ‚Äî%</span>
                        </div>
                        <div class="matrix-container">
                            <div id="cued-matrix" class="attention-matrix">
                                <div class="matrix-placeholder">Select a head to view attention</div>
                            </div>
                        </div>
                        <div class="stripe-analysis">
                            <h4>Top Source Sentences (Stripes) <span class="info-icon" onmouseenter="showInfoTooltip(event)" onmouseleave="hideTooltip()">‚ÑπÔ∏è</span></h4>
                            <ul id="cued-stripes" class="stripe-list">
                                <!-- Populated dynamically -->
                            </ul>
                        </div>
                    </div>

                    <!-- Uncued Panel -->
                    <div class="attention-panel">
                        <div class="panel-header">
                            <h3>Uncued Rollout</h3>
                            <span id="uncued-selected-head" class="selected-head-badge">L0-H0</span>
                        </div>
                        <div class="matrix-container">
                            <div id="uncued-matrix" class="attention-matrix">
                                <div class="matrix-placeholder">Select a head to view attention</div>
                            </div>
                        </div>
                        <div class="stripe-analysis">
                            <h4>Top Source Sentences (Stripes) <span class="info-icon" onmouseenter="showInfoTooltip(event)" onmouseleave="hideTooltip()">‚ÑπÔ∏è</span></h4>
                            <ul id="uncued-stripes" class="stripe-list">
                                <!-- Populated dynamically -->
                            </ul>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Summary for cued vs uncued -->
            <section class="conclusion-section">
                <h2>Summary</h2>
                <div class="summary-stats">
                    <div class="summary-card">
                        <span class="summary-value" id="shared-heads-count">0</span>
                        <span class="summary-label">Shared Heads among cued and uncued versions</span>
                    </div>
                    <div class="summary-card">
                        <span class="summary-value" id="shared-sources-count">0</span>
                        <span class="summary-label">Shared Source Sentences</span>
                    </div>
                    <div class="summary-card">
                        <span class="summary-value" id="cue-in-stripes">‚Äî</span>
                        <span class="summary-label">Cue in Top Sources (Cued)</span>
                    </div>
                </div>
            </section>
            </div> <!-- unified-attention-container -->

            <p class="viz-intro">Now I look at the faithful and unfaithful rollout examples for the cued only version of question ID <span id="fvu-question-id" class="highlight-id">‚Äî</span>, <span id="fvu-category-label" class="highlight-category">‚Äî</span></p>

            <!-- Faithful vs Unfaithful Comparison (NEW) -->
            <section id="fvu-section" class="attention-section fvu-section hidden">
                <h2>Faithful vs Unfaithful Comparison</h2>

                <div id="fvu-not-available" class="fvu-not-available hidden">
                    <p>Faithful vs Unfaithful comparison not available for this problem. 
                    This requires both a cued rollout that mentions the professor AND one that doesn't.</p>
                </div>
                
                <div id="fvu-comparison" class="attention-comparison">
                    <!-- Faithful Panel -->
                    <div class="attention-panel faithful-panel">
                        <div class="panel-header">
                            <h3>Faithful (Cued + Mentions)</h3>
                            <span id="fvu-faithful-head" class="selected-head-badge">L0-H0</span>
                        </div>
                        <div class="matrix-container">
                            <div id="faithful-matrix" class="attention-matrix">
                                <div class="matrix-placeholder">Select a head to view attention</div>
                            </div>
                        </div>
                        <div class="stripe-analysis">
                            <h4>Top Source Sentences <span class="info-icon" onmouseenter="showInfoTooltip(event)" onmouseleave="hideTooltip()">‚ÑπÔ∏è</span></h4>
                            <ul id="faithful-stripes" class="stripe-list">
                                <!-- Populated dynamically -->
                            </ul>
                        </div>
                    </div>

                    <!-- Unfaithful Panel -->
                    <div class="attention-panel unfaithful-panel">
                        <div class="panel-header">
                            <h3>Unfaithful (Cued + Silent)</h3>
                            <span id="fvu-unfaithful-head" class="selected-head-badge">L0-H0</span>
                        </div>
                        <div class="matrix-container">
                            <div id="unfaithful-matrix" class="attention-matrix">
                                <div class="matrix-placeholder">Select a head to view attention</div>
                            </div>
                        </div>
                        <div class="stripe-analysis">
                            <h4>Top Source Sentences <span class="info-icon" onmouseenter="showInfoTooltip(event)" onmouseleave="hideTooltip()">‚ÑπÔ∏è</span></h4>
                            <ul id="unfaithful-stripes" class="stripe-list">
                                <!-- Populated dynamically -->
                            </ul>
                        </div>
                    </div>
                </div>
            </section>
        </main>

        <!-- Future Directions -->
        <section class="future-directions-section">
            <h2>Future Directions</h2>
            <p>
                (1) Test on domains where post-hoc rationalization is common,
                such as the hiring-scenario dataset (<a href="https://arxiv.org/abs/2506.10922" target="_blank">Karvonen & Marks, 2025</a>), where models explain choices with
                acceptable narratives that may not reflect true decision factors.<br>
                (2) Combine with Verbalization Fine-Tuning (<a href="https://arxiv.org/abs/2506.22777" target="_blank">Turpin et al., 2025</a>) ‚Äî if VFT increases cue admission,
                do receiver heads shift to anchor on the verbalized cue sentence post-training?<br>
                (2.1) Try model diffing between VFT model and original.<br>
                Such experiments could validate whether attention markers reliably track cue-following behavior.
            </p>
        </section>

        <!-- Acknowledgements -->
        <section class="acknowledgements-section">
            <h2>Acknowledgements</h2>
            <p>
                This work was done under the <a href="https://researchathena.org/" target="_blank">ATHENA</a> fellowship with Arun Jose as mentor.
                I would also like to thank Qiyao Wei and Andy Wang for valuable advice, feedback, and discussions.
            </p>
        </section>

        <!-- Footer -->
        <footer class="footer">
            <p>Attention Analysis for Chain-of-Thought Faithfulness</p>
            <p class="footer-links">
                <a href="#methodology">Methodology</a> ¬∑
                <a href="https://github.com/yourusername/thought-anchors-faithfulness" target="_blank">GitHub</a>
            </p>
        </footer>
    </div>

    <!-- Tooltip -->
    <div id="tooltip" class="tooltip hidden"></div>

    <script src="data.js"></script>
    <script src="app.js"></script>
</body>
</html>

