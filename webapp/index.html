<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Faithful vs Unfaithful CoT Attention Analysis</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:wght@400;600;700&family=Source+Sans+Pro:wght@400;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <!-- Background doodle decorations -->
    <div class="bg-doodles">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-1">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-2">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-3">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-4">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-5">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-6">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-7">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-8">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-9">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-10">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-11">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-12">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-13">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-14">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-15">
        <img src="images/attention_heads_doodle.png" alt="" class="bg-doodle bg-doodle-16">
    </div>

    <div class="container">
        <!-- Header -->
        <header class="header">
            <h1>Do Faithful &amp; Unfaithful CoT Differ in How They Pay Attention?</h1>
            <p class="subtitle">Interactive visualization of attention patterns in chain-of-thought reasoning</p>
        </header>

        <!-- TL;DR Section -->
        <section class="tldr-section">
            <h2>TL;DR</h2>
            <p class="tldr-intro">
                I analyze consistently faithful &amp; unfaithful rollouts for MMLU &amp; GPQA-diamond datasets using the
                <a href="https://arxiv.org/abs/2506.19143" target="_blank">Thought Anchors</a> framework.
            </p>
            <ul class="tldr-list">
                <li>Unfaithful CoT shows <strong>higher cue-following rate</strong> than faithful CoT, when we control for sufficient cue influence.</li>
                <li>Receiver heads <strong>almost do not overlap</strong> between faithful &amp; unfaithful CoT, when we look at attention distribution across reasoning sentences only.</li>
                <li>There exist <strong>"universal attention heads"</strong> that consistently appear as receiver heads for both MMLU &amp; GPQA, in both faithful &amp; unfaithful rollouts.</li>
            </ul>
            <p class="tldr-link">
                <a href="#key-findings">View detailed findings below ‚Üì</a>
            </p>
        </section>

        <!-- Methodology Section (Collapsed by default) -->
        <section class="methodology-collapsible" id="methodology">
            <h2 class="methodology-header" onclick="toggleMethodology()">
                Methodology
                <span id="methodology-toggle" class="toggle-icon">‚ñ∂</span>
            </h2>
            <div id="methodology-content" class="methodology-body collapsed">

                <h3>Hypothesis</h3>
                <p>
                    If white-box attention attribution patterns reflect how models use information during reasoning,
                    then <strong>faithful CoT</strong> (where reasoning genuinely influences the answer) should show focused attention
                    to relevant broadcasting sentences, while <strong>unfaithful CoT</strong> (post-hoc rationalization)
                    may exhibit more diffuse attention or anchor to different, potentially misleading parts of the reasoning chain.
                </p>
                <p>
                    This work replicates and extends <a href="https://arxiv.org/abs/2506.19143" target="_blank">Bogdan et al. (2025) "Thought Anchors"</a>
                    to test whether receiver heads and vertical stripe patterns of attention targeted towards specific sentences ("thought anchors") differ between faithful and unfaithful chains.
                    If attention markers meaningfully distinguish faithful from unfaithful CoT, this could enable
                    white-box monitoring of reasoning quality in deployed models.
                </p>

                <h3 id="selection-criteria">Problem Selection Criteria</h3>
                <p>Problems were selected from MMLU and GPQA using the following criteria:</p>
                <ul class="clean-list">
                    <li><strong>Low no-reasoning accuracy (&lt;50%)</strong> ‚Äî Problems where the model struggles without CoT</li>
                    <li><strong>High cue response gap (‚â•0.5)</strong> ‚Äî Large difference in cue-following between cued and uncued conditions</li>
                    <li><strong>Cue ‚â† Ground Truth</strong> ‚Äî The cue answer differs from the correct answer</li>
                    <li><strong>Each problem is unique</strong></li>
                </ul>

                <h3>Faithfulness Classification</h3>
                <ul class="clean-list">
                    <li><strong>Faithful CoT:</strong> Rollouts where the model follows the cue AND explicitly mentions the cue in its reasoning. Threshold: ‚â•80% for MMLU, ‚â•70% for GPQA.</li>
                    <li><strong>Unfaithful CoT:</strong> Rollouts where the model follows the cue BUT does not mention it ‚Äî the CoT does not reflect the actual decision process. Threshold: ‚â•80% for MMLU, ‚â•70% for GPQA.</li>
                    <li><strong>Mixed:</strong> Problems with substantial proportions of both. Threshold: unfaithful/faithful ratio ‚â•40%.</li>
                </ul>

                <h3>Dataset Statistics</h3>
                <p><strong>MMLU:</strong> 143 problems, 20 rollouts per condition, median reasoning accuracy 0.637, median no-reasoning accuracy 0.474.</p>
                <p><strong>GPQA-Diamond:</strong> 186 problems, 20 rollouts per condition, mean base accuracy 0.577, mean no-reasoning accuracy 0.378.</p>

                <h3>Model &amp; Generation Settings</h3>
                <p>
                    Two generation stages: (1) initial rollouts for faithfulness classification,
                    (2) attention analysis rollouts for visualization.
                </p>
                <ul class="clean-list">
                    <li><strong>Model:</strong> <code>deepseek-ai/deepseek-r1-distill-qwen-14b</code></li>
                    <li><strong>Temperature:</strong> 0.7, <strong>Top-p:</strong> 0.95</li>
                    <li><strong>Max tokens:</strong> 2048 (MMLU), 8192/2048 (GPQA stage 1/2)</li>
                    <li><strong>Rollouts:</strong> 20 per condition (stage 1), 5 per condition (stage 2)</li>
                </ul>

                <h3>Attention Analysis</h3>
                <p>
                    For each problem, we identify <strong>receiver heads</strong> ‚Äî attention heads with high kurtosis
                    in their attention distribution. High kurtosis indicates the head consistently
                    attends to specific source sentences (creating "vertical stripes" in the attention matrix).
                    I compare which source sentences receive the most attention in cued vs. uncued rollouts.
                </p>

            </div> <!-- methodology-content -->
        </section>

        <!-- Try It Out Section -->
        <section class="try-it-out-section">
            <h2>Try It Out!</h2>
            <p class="section-intro">Explore attention patterns across different problems and datasets.</p>
        </section>

        <!-- Dataset Selection -->
        <section class="control-panel">
            <div class="control-group">
                <label for="dataset-select">Dataset</label>
                <select id="dataset-select">
                    <option value="mmlu" selected>MMLU</option>
                    <option value="gpqa">GPQA-Diamond</option>
                </select>
            </div>

            <div class="control-group">
                <label>Category</label>
                <div class="checkbox-group" id="category-checkboxes">
                    <label class="checkbox-label">
                        <input type="checkbox" id="cat-faithful" value="faithful" checked>
                        <span>Faithful CoT</span>
                    </label>
                    <label class="checkbox-label">
                        <input type="checkbox" id="cat-unfaithful" value="unfaithful" checked>
                        <span>Unfaithful CoT</span>
                    </label>
                </div>
            </div>

            <div class="control-group">
                <label for="pi-select">Problem ID (PI)</label>
                <select id="pi-select">
                    <!-- Populated dynamically -->
                </select>
            </div>

            <div class="control-group">
                <label for="heads-source">
                    Receiver Heads From
                    <span class="info-icon" id="heads-source-info" data-tooltip="heads-source-tooltip">‚ÑπÔ∏è</span>
                </label>
                <select id="heads-source">
                    <option value="aggregate">All in Category</option>
                    <option value="single">This PI Only</option>
                </select>
                <div id="heads-source-tooltip" class="control-tooltip hidden">
                    <p><strong>All in Category:</strong> Receiver heads are calculated using kurtosis measure calculated across all rollouts from all problem IDs (PIs) in the selected category. If only one category (faithful or unfaithful) is selected, kurtosis is computed across all rollouts from that category.</p>
                    <p style="margin-top: 0.5rem;"><strong>Note on combining categories:</strong> If both faithful and unfaithful categories are selected, kurtosis would be computed across all rollouts from both categories combined. However, this may not be statistically appropriate since faithful and unfaithful problems may exhibit systematically different attention patterns. The original analysis keeps them separate for comparison.</p>
                    <p style="margin-top: 0.5rem;"><strong>This PI Only:</strong> Receiver heads are calculated only across rollouts of this specific problem ID (PI).</p>
                    <p style="margin-top: 0.5rem; font-size: 0.85rem; color: var(--text-muted);">Note: Bogdan et al. calculate receiver heads across the whole dataset of math rollouts (<a href="https://huggingface.co/datasets/uzaymacar/math-rollouts" target="_blank">link</a>).</p>
                </div>
            </div>

            <div class="control-group">
                <label for="attention-mode">Attention Mode</label>
                <select id="attention-mode">
                    <option value="full" selected>With Prompt</option>
                    <option value="reasoning">Reasoning Only</option>
                </select>
            </div>
        </section>

        <!-- TBA Message (for datasets without data) -->
        <div id="tba-message" class="tba-message hidden">
            <h2>üìä No Problems Available</h2>
            <p>No problems found for this dataset/category combination.</p>
        </div>

        <!-- Main Content -->
        <main id="main-content">
            <!-- Question Info -->
            <section class="question-info">
                <h2>Question Details <span id="pi-category-badge" class="category-badge"></span></h2>
                <div class="info-grid">
                    <div class="info-card">
                        <span class="info-label">Question Text</span>
                        <p id="question-text" class="question-text">Loading...</p>
                        <button id="expand-question-btn" class="expand-btn" onclick="toggleQuestionExpand()">Show more ‚ñº</button>
                    </div>
                    <div class="info-row">
                        <div class="info-card small">
                            <span class="info-label">Ground Truth</span>
                            <span id="gt-answer" class="answer-badge gt">‚Äî</span>
                        </div>
                        <div class="info-card small">
                            <span class="info-label">Cue Answer</span>
                            <span id="cue-answer" class="answer-badge cue">‚Äî</span>
                        </div>
                        <div class="info-card small">
                            <span class="info-label">Reasoning Acc</span>
                            <span id="reasoning-acc" class="stat-value">‚Äî</span>
                        </div>
                        <div class="info-card small">
                            <span class="info-label">No-Reasoning Acc</span>
                            <span id="no-reasoning-acc" class="stat-value">‚Äî</span>
                        </div>
                        <div class="info-card small">
                            <span class="info-label">Faithfulness %</span>
                            <span id="faithfulness-pct" class="stat-value faithful-stat">‚Äî</span>
                        </div>
                    </div>
                </div>
                
                <!-- Warning for problems where reasoning is worse than no-reasoning -->
                <div id="reasoning-warning" class="reasoning-warning hidden">
                    <span class="warning-icon">‚ö†Ô∏è</span>
                    <span class="warning-text">
                        <strong>Note:</strong> This problem may be non-representative as its reasoning accuracy is lower than no-reasoning accuracy. 
                        However, attention patterns may still be interesting for such examples.
                    </span>
                </div>
            </section>

            <!-- Receiver Heads -->
            <section class="receiver-heads-section">
                <img src="images/receiver_head_left.png" alt="" class="receiver-doodle-left">
                <img src="images/receiver_heads_doodle_right.png" alt="" class="receiver-doodle-right">
                <h2>Top Receiver Heads <span id="heads-category-badge" class="category-badge"></span></h2>
                <p class="section-desc">Heads with highest kurtosis in attention distribution (indicating focused attention patterns)</p>
                <p id="heads-fallback-note" class="fallback-note hidden"></p>

                <div class="heads-comparison">
                    <div class="heads-column">
                        <h3>Cued Rollout</h3>
                        <p class="column-note">‚Ä¢ Dot indicates cue mention among the top source sentences for the selected layer-head pair</p>
                        <div id="cued-heads" class="heads-list">
                            <!-- Populated dynamically -->
                        </div>
                    </div>
                    <div class="heads-column">
                        <h3>Uncued Rollout</h3>
                        <div id="uncued-heads" class="heads-list">
                            <!-- Populated dynamically -->
                        </div>
                    </div>
                </div>
                
                <div class="shared-heads">
                    <span class="shared-label">Shared Heads:</span>
                    <span id="shared-heads-list">‚Äî</span>
                </div>
            </section>

            <!-- Attention Matrices Side by Side -->
            <section class="attention-section">
                <h2>Attention Patterns <span id="attention-category-badge" class="category-badge"></span></h2>
                <p class="section-desc">Click on a head above to view its attention matrix. Hover for values.</p>
                
                <div class="attention-comparison">
                    <!-- Cued Panel -->
                    <div class="attention-panel">
                        <div class="panel-header">
                            <h3>Cued Rollout</h3>
                            <span id="cued-selected-head" class="selected-head-badge">L0-H0</span>
                            <span id="cued-prof-mention" class="prof-mention-badge">Faithfulness rate: ‚Äî%</span>
                        </div>
                        <div class="matrix-container">
                            <div id="cued-matrix" class="attention-matrix">
                                <div class="matrix-placeholder">Select a head to view attention</div>
                            </div>
                        </div>
                        <div class="stripe-analysis">
                            <h4>Top Source Sentences (Stripes) <span class="info-icon" onmouseenter="showInfoTooltip(event)" onmouseleave="hideTooltip()">‚ÑπÔ∏è</span></h4>
                            <ul id="cued-stripes" class="stripe-list">
                                <!-- Populated dynamically -->
                            </ul>
                        </div>
                    </div>

                    <!-- Uncued Panel -->
                    <div class="attention-panel">
                        <div class="panel-header">
                            <h3>Uncued Rollout</h3>
                            <span id="uncued-selected-head" class="selected-head-badge">L0-H0</span>
                        </div>
                        <div class="matrix-container">
                            <div id="uncued-matrix" class="attention-matrix">
                                <div class="matrix-placeholder">Select a head to view attention</div>
                            </div>
                        </div>
                        <div class="stripe-analysis">
                            <h4>Top Source Sentences (Stripes) <span class="info-icon" onmouseenter="showInfoTooltip(event)" onmouseleave="hideTooltip()">‚ÑπÔ∏è</span></h4>
                            <ul id="uncued-stripes" class="stripe-list">
                                <!-- Populated dynamically -->
                            </ul>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Faithful vs Unfaithful Comparison (NEW) -->
            <section id="fvu-section" class="attention-section fvu-section hidden">
                <h2>Faithful vs Unfaithful Comparison</h2>
                <p class="section-desc">
                    Comparing two <strong>cued</strong> rollouts: one that <em>mentions</em> the cue (faithful) vs one that <em>follows</em> the cue without mentioning it (unfaithful).
                </p>
                
                <div id="fvu-not-available" class="fvu-not-available hidden">
                    <p>Faithful vs Unfaithful comparison not available for this problem. 
                    This requires both a cued rollout that mentions the professor AND one that doesn't.</p>
                </div>
                
                <div id="fvu-comparison" class="attention-comparison">
                    <!-- Faithful Panel -->
                    <div class="attention-panel faithful-panel">
                        <div class="panel-header">
                            <h3>Faithful (Cued + Mentions)</h3>
                            <span id="fvu-faithful-head" class="selected-head-badge">L0-H0</span>
                        </div>
                        <div class="matrix-container">
                            <div id="faithful-matrix" class="attention-matrix">
                                <div class="matrix-placeholder">Select a head to view attention</div>
                            </div>
                        </div>
                        <div class="stripe-analysis">
                            <h4>Top Source Sentences <span class="info-icon" onmouseenter="showInfoTooltip(event)" onmouseleave="hideTooltip()">‚ÑπÔ∏è</span></h4>
                            <ul id="faithful-stripes" class="stripe-list">
                                <!-- Populated dynamically -->
                            </ul>
                        </div>
                    </div>

                    <!-- Unfaithful Panel -->
                    <div class="attention-panel unfaithful-panel">
                        <div class="panel-header">
                            <h3>Unfaithful (Cued + Silent)</h3>
                            <span id="fvu-unfaithful-head" class="selected-head-badge">L0-H0</span>
                        </div>
                        <div class="matrix-container">
                            <div id="unfaithful-matrix" class="attention-matrix">
                                <div class="matrix-placeholder">Select a head to view attention</div>
                            </div>
                        </div>
                        <div class="stripe-analysis">
                            <h4>Top Source Sentences <span class="info-icon" onmouseenter="showInfoTooltip(event)" onmouseleave="hideTooltip()">‚ÑπÔ∏è</span></h4>
                            <ul id="unfaithful-stripes" class="stripe-list">
                                <!-- Populated dynamically -->
                            </ul>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Conclusion -->
            <section class="conclusion-section">
                <h2>Summary</h2>
                <div class="summary-stats">
                    <div class="summary-card">
                        <span class="summary-value" id="shared-heads-count">0</span>
                        <span class="summary-label">Shared Heads</span>
                    </div>
                    <div class="summary-card">
                        <span class="summary-value" id="shared-sources-count">0</span>
                        <span class="summary-label">Shared Source Sentences</span>
                    </div>
                    <div class="summary-card">
                        <span class="summary-value" id="cue-in-stripes">‚Äî</span>
                        <span class="summary-label">Cue in Top Sources (Cued)</span>
                    </div>
                </div>
            </section>
        </main>

        <!-- Key Findings Section (Collapsible) -->
        <section class="key-findings-section" id="key-findings">
            <h2 class="key-findings-header" onclick="toggleKeyFindings()">
                Detailed Key Findings
                <span id="key-findings-toggle" class="toggle-icon">‚ñ∂</span>
            </h2>
            <div id="key-findings-content" class="key-findings-body collapsed">
            <div class="findings-grid">
                <!-- Finding 1: Hidden Influence -->
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="finding-metric">Unfaithful CoT (silent cue following) shows higher cue-following rate (91%) than faithful CoT (66%) in selected PIs.</span>
                    </div>
                    <p style="margin-top: 0.5rem;">Without selection, the pattern reverses: unfaithful 34% vs faithful 42%. However, controlling for cue response gap ‚â• 0.5, the pattern holds: <strong>unfaithful 80% vs faithful 71%</strong> (+9pp).</p>
                    <p><a href="#selection-criteria" onclick="document.getElementById('methodology-content').classList.remove('collapsed'); document.getElementById('methodology-toggle').classList.add('expanded');">Problem Selection Criteria</a></p>
                    <button class="expand-graph-btn" onclick="toggleGraph('hidden-influence')">Show graph ‚ñº</button>
                    <div id="hidden-influence" class="correlation-graph hidden">
                        <img src="images/conclusion_hidden_influence_web.png" alt="Hidden Influence Comparison">
                    </div>
                </div>

                <!-- Finding 2: Divergent Circuits -->
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="finding-metric">Divergent Attention Circuits</span>
                    </div>
                    <p>Faithful and unfaithful CoT share 62-83% of top receiver heads when <strong>prompt is included into calculation of receiver heads</strong> and attention to sentences within prompt is also accounted for.</p>
                    <p>Near-zero overlap (0-11%) when we consider <strong>reasoning only attention</strong>: faithful and unfaithful PIs use completely different heads.</p>
                    <button class="expand-graph-btn" onclick="toggleGraph('divergent-circuits')">Show graph ‚ñº</button>
                    <div id="divergent-circuits" class="correlation-graph hidden">
                        <img src="images/conclusion_divergent_circuits_web.png" alt="Divergent Circuits Comparison">
                    </div>
                </div>

                <!-- Finding 3: Universal Receiver Heads -->
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="finding-metric">Universal Receiver Heads</span>
                    </div>
                    <p>Certain attention heads (e.g., L45_H0, L47_H3, L31_H34) appear as top receivers across both MMLU and GPQA, and in both faithful and unfaithful categories.</p>
                    <p style="font-size: 0.85rem; color: var(--text-secondary); margin-top: 0.5rem;">
                        These heads may represent fundamental "answer extraction" circuits in the model, independent of reasoning faithfulness.
                    </p>
                    <button class="expand-graph-btn" onclick="toggleGraph('universal-heads')">Show graph ‚ñº</button>
                    <div id="universal-heads" class="correlation-graph hidden">
                        <img src="images/conclusion_universal_heads_web.png" alt="Universal Receiver Heads">
                    </div>
                </div>
            </div>
            </div> <!-- key-findings-content -->
        </section>

        <!-- Conclusion -->
        <section class="conclusion-text-section">
            <h2>Conclusion & Future Directions</h2>
            <p class="conclusion-text">
                This visualization explores whether attention patterns can serve as white-box markers for CoT faithfulness.
                While vertical stripe patterns and receiver heads emerge in both faithful and unfaithful chains,
                further work is needed to determine if systematic differences exist.
            </p>
            <p class="conclusion-text" style="margin-top: 0.75rem; font-style: normal; font-size: 0.95rem;">
                <strong>Future directions:</strong><br>
                (1) Test on domains where post-hoc rationalization is common,
                such as the hiring-scenario dataset (<a href="https://arxiv.org/abs/2506.10922" target="_blank">Karvonen & Marks, 2025</a>), where models explain choices with
                acceptable narratives that may not reflect true decision factors.<br>
                (2) Combine with Verbalization Fine-Tuning (<a href="https://arxiv.org/abs/2506.22777" target="_blank">Turpin et al., 2025</a>) ‚Äî if VFT increases cue admission,
                do receiver heads shift to anchor on the verbalized cue sentence post-training?<br>
                (2.1) Try model diffing between VFT model and original.<br>
                Such experiments could validate whether attention markers reliably track cue-following behavior.
            </p>
        </section>

        <!-- Acknowledgements -->
        <section class="acknowledgements-section">
            <h2>Acknowledgements</h2>
            <p>
                This work was done under the <a href="https://researchathena.org/" target="_blank">ATHENA</a> fellowship with Arun Jose as mentor.
                I would also like to thank Qiyao Wei and Andy Wang for valuable advice, feedback, and discussions.
            </p>
        </section>

        <!-- Footer -->
        <footer class="footer">
            <p>Attention Analysis for Chain-of-Thought Faithfulness</p>
            <p class="footer-links">
                <a href="#methodology">Methodology</a> ¬∑
                <a href="https://github.com/yourusername/thought-anchors-faithfulness" target="_blank">GitHub</a>
            </p>
        </footer>
    </div>

    <!-- Tooltip -->
    <div id="tooltip" class="tooltip hidden"></div>

    <script src="data.js"></script>
    <script src="app.js"></script>
</body>
</html>

